\begin{abstract}
We consider deep neural networks (DNNs) with \emph{rectified linear units} (ReLUs). For such DNNs, we propose a \emph{pedagogical nugget} by combining \emph{neural tangent kernel} (NTK) and \emph{dual view}. Our aim is to explain training, generalisation, roles of weights/activation/width/depth. It is known that training a randomly initialised infinite width DNN is equivalent to a kernel method with the NTK at initialisation. Recent work based on dual view exploited the gating property of ReLU to show that  (i) most information is in the gates, (ii) learning in gates explains why finite width DNNs perform better than infinite width NTK. Dual view also simplified the NTK into a \emph{neural path kernel} (NPK) which is solely based on the gates.\\
In this paper, we show analytically that the NPK simplifies into \emph{atomic} units:  basic unit is the ReLU which acts as a gate; stacking ReLUs widthwise in a layer gives rise to a base kernel which measures the \emph{average} number of triggered ReLUs; stacking layers depthwise gives rise to the \emph{product} of base kernels.
 Thus, the NPK of a fully connected DNN involves a product of base kernels. In the case of convolutional layers with pooling, the NPK has a rotation invariance property. And in residual networks with skip connects, the NPK has a sum of product of base kernels structure. The complete picture is that a DNN with ReLUs can be seen as a \emph{composite kernel} whose base kernels are based on gates: training/generalisation is explained by NTK; role of weights and activations is explained by prior work on NPK; the roles of depth/width/convolutions/pooling/skip connection is explained in this paper. We support our theory via novel ablation studies.
\end{abstract}
