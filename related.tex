\section{Related and Future Work}
\setcitestyle{authoryear}
Our paper extended the work of \cite{ch2020neural} to CNN and ResNet. Further, we pointed out the \emph{composite} nature of the underlying kernel. Experiments with permuted masks and constant inputs are also significant and novel evidences, which to our knowledge are first of their kind in literature.  Gated linearity was studied recently by \cite{sss}, however, they considered only single layered gated networks.\cite{ntk,arora2019exact,cao2019generalization,jacot2019freeze,dudnn} have all used the NTK framework to understand questions related to optimisation and/or generalisation in DNNs. We now discuss the future work below.

1.  \emph{Base Kernel:} %We now informally reason out the behaviour of the base kernel $H^{\text{lyr}}_{l,\Theta_0}$ as a function of $l$. 
At randomised initialisation, for each $l$, $\frac{H^{\text{lyr}}_{l,\Theta_0}(s,s')}{w}$ is the fraction of gates that are simultaneously active for input examples $s,s'$, which in the limit of infinite width is equal to $\left(\frac{1}2- \text{angle}(z_{x_s}(\cdot,l), z_{x_{s'}}(\cdot,l))\right)$ \citep{angle}. Further, due to the property of ReLU to pass only positive components, we conjecture that the pairwise angle between input examples measured at the hidden layer outputs is a decreasing function of depth and as $l\ra\infty, \frac{H^{\text{lyr}}_{l,\Theta_0}(s,s')}{w}\ra\frac{1}{2}, \forall s,s'\in[n]$.  We reserve a formal statement on the behaviour of $H^{\text{lyr}}_{l,\Theta_0}$ for the future.

2. \emph{Multiple Kernel Learning} \citep{mkl1,mkl2,mkl3,mkl4} is the name given to a class of methods that learn a linear or non-linear combination of one or many base kernels. For instance, \cite{mkl4} consider polynomial combinations of base kernels, which also has a `sum of products' form.  Our experiments do indicate that the learning in the gates (and hence the underlying base kernels) has a significant impact. Understanding $\kf$ (\Cref{prop:ntks}) might be a way to establish the extent and nature of kernel learning in deep learning. It is also interesting to check if in ResNet the kernels of its sub-FC-DNNs are combined optimally. 
% \citenum{mkl1,mkl2,mkl3,mkl4}
\section{Conclusion}
We attributed the success of deep learning to the following two key ingredients: (i) a composite kernel with gates as fundamental building blocks and (ii) allowing the gates to learn/adapt during training.  We justified our claims theoretically as well as experimentally. This work along with that of \cite{ch2020neural} provides a paradigm shift in understanding deep learning. Here, gates play a central role. Each gate is related to a hyper-plane, and gates together form layer level binary features whose kernels are the base kernels. Laying out these binary features depth-wise gives rise to a product of the base kernels. The skip connections gives a `sum of product' structure, and convolution with pooling gives rotation invariance. The learning in the gates further enhances the generalisation capabilities of the models. 

